import { useMutation, useQueryClient } from '@tanstack/react-query';
import { supabase } from '@/integrations/supabase/client';
import { ParsedCSVRow, parseNumericValue, parseDateValue, findColumn } from '@/utils/csvParser';
import type { Database } from '@/integrations/supabase/types';

type CakeSize = Database['public']['Enums']['cake_size'];
type CakeVariety = Database['public']['Enums']['cake_variety'];

interface ProductionOutput {
  size: CakeSize;
  variety: CakeVariety;
  quantity: number;
}

interface TransformedRun {
  run_date: string;
  doughs_produced: number;
  notes: string | null;
  outputs: ProductionOutput[];
}

interface ImportResult {
  success: boolean;
  imported: number;
  skipped: number;
  errors: string[];
  duplicates: string[];
}

// Column name mappings for flexibility
const DATE_COLUMNS = ['date', 'run_date', 'production date'];
const DOUGH_COLUMNS = ['total dough eq', 'doughs', 'dough eq', 'total dough'];
const MINI_COLUMNS = ['mini'];
const MEDIUM_TRAD_COLUMNS = ['medium traditional', 'med traditional', 'medium trad'];
const LARGE_TRAD_COLUMNS = ['large traditional', 'large trad'];
const FILLED_COLUMNS = ['glazed', 'creamcheese', 'cream cheese', 'bavarian', 'filled', 'medium glazed'];
const NOTES_COLUMNS = ['notes', 'note', 'comments'];

function transformCSVRow(row: ParsedCSVRow, headers: string[]): TransformedRun | null {
  // Find date column
  const dateCol = findColumn(headers, DATE_COLUMNS);
  if (!dateCol) return null;
  
  const dateValue = parseDateValue(row[dateCol]);
  if (!dateValue) return null; // Skip OFF days and invalid dates
  
  // Find dough column
  const doughCol = findColumn(headers, DOUGH_COLUMNS);
  const doughsProduced = doughCol ? parseNumericValue(row[doughCol]) : 0;
  
  // Parse outputs
  const outputs: ProductionOutput[] = [];
  
  // Mini traditional
  const miniCol = findColumn(headers, MINI_COLUMNS);
  if (miniCol) {
    const qty = parseNumericValue(row[miniCol]);
    if (qty > 0) {
      outputs.push({ size: 'mini', variety: 'traditional', quantity: Math.round(qty) });
    }
  }
  
  // Medium traditional
  const medTradCol = findColumn(headers, MEDIUM_TRAD_COLUMNS);
  if (medTradCol) {
    const qty = parseNumericValue(row[medTradCol]);
    if (qty > 0) {
      outputs.push({ size: 'medium', variety: 'traditional', quantity: Math.round(qty) });
    }
  }
  
  // Large traditional
  const largeTradCol = findColumn(headers, LARGE_TRAD_COLUMNS);
  if (largeTradCol) {
    const qty = parseNumericValue(row[largeTradCol]);
    if (qty > 0) {
      outputs.push({ size: 'large', variety: 'traditional', quantity: Math.round(qty) });
    }
  }
  
  // Sum all filled varieties into one medium filled entry
  let filledTotal = 0;
  for (const header of headers) {
    const headerLower = header.toLowerCase();
    if (FILLED_COLUMNS.some(fc => headerLower.includes(fc))) {
      filledTotal += parseNumericValue(row[header]);
    }
  }
  if (filledTotal > 0) {
    outputs.push({ size: 'medium', variety: 'filled', quantity: Math.round(filledTotal) });
  }
  
  // Skip if no outputs
  if (outputs.length === 0 && doughsProduced === 0) return null;
  
  // Get notes
  const notesCol = findColumn(headers, NOTES_COLUMNS);
  const notes = notesCol ? row[notesCol] || null : null;
  
  return {
    run_date: dateValue,
    doughs_produced: doughsProduced,
    notes,
    outputs
  };
}

async function importProductionRuns(
  rows: ParsedCSVRow[],
  headers: string[]
): Promise<ImportResult> {
  const result: ImportResult = {
    success: true,
    imported: 0,
    skipped: 0,
    errors: [],
    duplicates: []
  };

  // Transform all rows
  const transformedRuns: TransformedRun[] = [];
  for (const row of rows) {
    const transformed = transformCSVRow(row, headers);
    if (transformed) {
      transformedRuns.push(transformed);
    } else {
      result.skipped++;
    }
  }

  // Check for existing dates
  const dates = transformedRuns.map(r => r.run_date);
  const { data: existingRuns } = await supabase
    .from('production_runs')
    .select('run_date')
    .in('run_date', dates);

  const existingDates = new Set(existingRuns?.map(r => r.run_date) || []);

  // Filter out duplicates
  const runsToImport = transformedRuns.filter(run => {
    if (existingDates.has(run.run_date)) {
      result.duplicates.push(run.run_date);
      return false;
    }
    return true;
  });

  // Import runs one by one to get IDs for outputs
  for (const run of runsToImport) {
    try {
      // Insert production run (batch_number is auto-generated by trigger, but we provide placeholder for type safety)
      const { data: insertedRun, error: runError } = await supabase
        .from('production_runs')
        .insert([{
          batch_number: 'TEMP', // Will be overwritten by generate_batch_number trigger
          run_date: run.run_date,
          doughs_produced: run.doughs_produced,
          notes: run.notes
        }])
        .select('id')
        .single();

      if (runError) {
        result.errors.push(`Failed to import run for ${run.run_date}: ${runError.message}`);
        continue;
      }

      // Insert outputs
      if (run.outputs.length > 0 && insertedRun) {
        const outputsToInsert = run.outputs.map(output => ({
          run_id: insertedRun.id,
          size: output.size,
          variety: output.variety,
          quantity_produced: output.quantity,
          quantity_sold: 0
        }));

        const { error: outputError } = await supabase
          .from('production_run_outputs')
          .insert(outputsToInsert);

        if (outputError) {
          result.errors.push(`Failed to import outputs for ${run.run_date}: ${outputError.message}`);
        }
      }

      result.imported++;
    } catch (error) {
      result.errors.push(`Error importing ${run.run_date}: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  result.success = result.errors.length === 0;
  return result;
}

export function useImportProductionRuns() {
  const queryClient = useQueryClient();

  return useMutation({
    mutationFn: ({ rows, headers }: { rows: ParsedCSVRow[]; headers: string[] }) =>
      importProductionRuns(rows, headers),
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ['production-runs'] });
    }
  });
}
